{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7823e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Move one level up to 'project_root'\n",
    "from scripts.utilities import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linearmodels.panel import PanelOLS\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec1bb751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INCR</th>\n",
       "      <th>EURN</th>\n",
       "      <th>NRP</th>\n",
       "      <th>AUB</th>\n",
       "      <th>ROIV</th>\n",
       "      <th>RBA</th>\n",
       "      <th>SBGI</th>\n",
       "      <th>EBON</th>\n",
       "      <th>JCTCF</th>\n",
       "      <th>EXAS</th>\n",
       "      <th>...</th>\n",
       "      <th>PFC</th>\n",
       "      <th>GM</th>\n",
       "      <th>BOTJ</th>\n",
       "      <th>GROV</th>\n",
       "      <th>DRH</th>\n",
       "      <th>ODD</th>\n",
       "      <th>SITM</th>\n",
       "      <th>CPBI</th>\n",
       "      <th>VRTS</th>\n",
       "      <th>TEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358566</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>-0.042308</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>-0.020725</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.051967</td>\n",
       "      <td>0.434615</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>-0.160952</td>\n",
       "      <td>-0.001287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.120235</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>-0.002551</td>\n",
       "      <td>-0.011364</td>\n",
       "      <td>-0.006024</td>\n",
       "      <td>0.037681</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.051111</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-0.021739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>-0.005254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015100</td>\n",
       "      <td>-0.017544</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349603</td>\n",
       "      <td>-0.042525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>-0.025576</td>\n",
       "      <td>0.057472</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>-0.027933</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>-0.160677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009009</td>\n",
       "      <td>-0.024355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054985</td>\n",
       "      <td>-0.049861</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>-0.138772</td>\n",
       "      <td>-0.012113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.051219</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>0.048850</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>0.100756</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.013393</td>\n",
       "      <td>-0.027783</td>\n",
       "      <td>-0.008163</td>\n",
       "      <td>-0.007718</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>-0.020436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>-0.062761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004386</td>\n",
       "      <td>-0.029126</td>\n",
       "      <td>-0.108508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>-0.009523</td>\n",
       "      <td>0.069542</td>\n",
       "      <td>-0.043127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.020178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015968</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>-0.030837</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.098202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.017037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>-0.012238</td>\n",
       "      <td>-0.021125</td>\n",
       "      <td>-0.012676</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>-0.026465</td>\n",
       "      <td>0.018034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>-0.017543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031818</td>\n",
       "      <td>-0.044335</td>\n",
       "      <td>-0.080605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.066932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.024451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004103</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>-0.004765</td>\n",
       "      <td>-0.063718</td>\n",
       "      <td>-0.005837</td>\n",
       "      <td>-0.019417</td>\n",
       "      <td>0.019143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>-0.009492</td>\n",
       "      <td>-0.014115</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>-0.014084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>-0.023794</td>\n",
       "      <td>-0.023809</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.014965</td>\n",
       "      <td>0.063081</td>\n",
       "      <td>0.057389</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.019142</td>\n",
       "      <td>-0.014017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>-0.041237</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>-0.024390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007805</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>-0.039385</td>\n",
       "      <td>-0.002930</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.005687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         INCR      EURN       NRP       AUB      ROIV       RBA      SBGI  \\\n",
       "1         NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2    0.358566  0.009836  0.010309  0.100001 -0.042308  0.020711 -0.020725   \n",
       "3   -0.120235 -0.017857 -0.002551 -0.011364 -0.006024  0.037681  0.005291   \n",
       "4   -0.033333  0.016529 -0.025576  0.057472  0.001919 -0.027933  0.010526   \n",
       "5    0.000000 -0.051219  0.004725  0.000000  0.008166  0.048850 -0.020833   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "96   0.000000  0.004008  0.004349 -0.062761  0.000000 -0.004386 -0.029126   \n",
       "97   0.000000 -0.015968  0.004764  0.017857  0.001019 -0.030837  0.015000   \n",
       "98  -0.150000 -0.002704  0.007759 -0.017543  0.000000 -0.031818 -0.044335   \n",
       "99   0.029412 -0.009492 -0.014115  0.071428  0.004073 -0.014084  0.000000   \n",
       "100  0.142857  0.008898  0.019523 -0.066667  0.009128  0.009524 -0.041237   \n",
       "\n",
       "         EBON     JCTCF      EXAS  ...       PFC        GM      BOTJ  \\\n",
       "1         NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "2   -0.100000  0.000000 -0.041667  ...  0.000000  0.002048  0.000000   \n",
       "3    0.051111 -0.031250 -0.021739  ...  0.009091 -0.005254  0.000000   \n",
       "4   -0.160677  0.000000 -0.004444  ... -0.009009 -0.024355  0.000000   \n",
       "5    0.100756  0.032258  0.000000  ...  0.018182  0.006918  0.090909   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "96  -0.108508  0.000000 -0.015018  ...  0.008849  0.000000  0.000000   \n",
       "97   0.098202  0.000000  0.125561  ... -0.052632 -0.017037  0.000000   \n",
       "98  -0.080605  0.000000 -0.066932  ...  0.000000 -0.024451  0.000000   \n",
       "99   0.008219  0.000000  0.031597  ... -0.027778 -0.023794 -0.023809   \n",
       "100  0.031250  0.000000 -0.002483  ...  0.028572  0.012350 -0.024390   \n",
       "\n",
       "         GROV       DRH       ODD      SITM      CPBI      VRTS       TEL  \n",
       "1         NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "2    0.015228  0.075472  0.051967  0.434615 -0.005495 -0.160952 -0.001287  \n",
       "3   -0.015100 -0.017544  0.051200 -0.032172  0.000000  0.349603 -0.042525  \n",
       "4    0.000000  0.000000 -0.054985 -0.049861  0.002210 -0.138772 -0.012113  \n",
       "5    0.000000 -0.013393 -0.027783 -0.008163 -0.007718 -0.032227 -0.020436  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "96   0.001028 -0.009523  0.069542 -0.043127  0.000000  0.014706  0.020178  \n",
       "97   0.001541 -0.012238 -0.021125 -0.012676  0.002927 -0.026465  0.018034  \n",
       "98  -0.004103  0.005309 -0.004765 -0.063718 -0.005837 -0.019417  0.019143  \n",
       "99   0.001030  0.014965  0.063081  0.057389  0.001957  0.019142 -0.014017  \n",
       "100  0.000000  0.007805 -0.001324 -0.039385 -0.002930  0.017487  0.005687  \n",
       "\n",
       "[100 rows x 5468 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = pd.read_csv('../data/returns.csv', index_col=0).iloc[:100]\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba16d3f",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d42f0",
   "metadata": {},
   "source": [
    "The first approach is to shuffle the dataset multiple times to make train/test folds, then construct the models for mean and variance on the train subsample and assess the results on the test subsample.  \n",
    "Test size will be 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db50974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsamples(data: pd.DataFrame, train_size : float = 0.7, shuffle: bool = True, random_state: int = None):\n",
    "\n",
    "    if shuffle: data = data.sample(frac=1, axis=1, random_state=random_state)   \n",
    "    data_train = data.iloc[:, :int(data.shape[1] * train_size)]\n",
    "    data_test = data.iloc[:, int(data.shape[1] * train_size):]\n",
    "\n",
    "    return data_train, data_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbc33c",
   "metadata": {},
   "source": [
    "### Pooled Diebold–Mariano Test for Equal Predictive Accuracy\n",
    "\n",
    "Let $L_m$ denote the pooled average squared-error loss for forecasting model $m$, computed over $n$ cross-sectional units and $T$ time periods:\n",
    "\n",
    "$$\n",
    "L_m = \\frac{1}{nT} \\sum_{t=1}^{T} \\sum_{i=1}^{n} \\left( y_{i,t} - \\hat{y}_{i,t}^{(m)} \\right)^2\n",
    "$$\n",
    "\n",
    "#### Hypotheses\n",
    "\n",
    "- **Null Hypothesis ($H_0$):** The expected pooled average loss is equal for both models:\n",
    "  $$\n",
    "  H_0: \\mathbb{E}[L_{m_1}] = \\mathbb{E}[L_{m_2}]\n",
    "  $$\n",
    "\n",
    "- **Alternative Hypothesis ($H_1$):** The expected pooled average loss of the unrestricted model is smaller:\n",
    "  $$\n",
    "  H_1: \\mathbb{E}[L_{m_1}] > \\mathbb{E}[L_{m_2}]\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $m_2$ is unrestricted model: $R_t = \\beta_1 + \\gamma_t + u_{it}$\n",
    "- $m_1$ is restricted model: $R_t = \\beta_2 + v_{it}$\n",
    "\n",
    "#### Test Statistic\n",
    "\n",
    "Define the loss differential for unit $i$ at time $t$ as:\n",
    "$$\n",
    "\\Delta L_{i,t} = \\left( y_{i,t} - \\hat{y}_{i,t}^{(m_1)} \\right)^2 - \\left( y_{i,t} - \\hat{y}_{i,t}^{(m_2)} \\right)^2\n",
    "$$\n",
    "\n",
    "Compute the cross-sectional average loss differential at each time $t$:\n",
    "$$\n",
    "\\Delta L_t = \\frac{1}{n} \\sum_{i=1}^{n} \\Delta L_{i,t}\n",
    "$$\n",
    "\n",
    "Scale these to obtain:\n",
    "$$\n",
    "\\hat{L}_t = \\sqrt{n} \\cdot \\Delta L_t\n",
    "$$\n",
    "\n",
    "The test statistic is then:\n",
    "$$\n",
    "J_{st}= \\frac{1}{\\sqrt{T}} \\sum_{t=1}^{T} \\frac{\\hat{L}_t - \\bar{{\\hat{L}}}}{\\hat{\\sigma}_{\\hat{L}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\bar{\\hat{L}} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{L}_t$ is the average of the $\\hat{L}_t$ series,\n",
    "- $\\hat{\\sigma}_{\\hat{L}}$ is a consistent estimator of the standard deviation of $\\hat{L}_t$, which can be computed using the Newey–West estimator to account for potential autocorrelation.\n",
    "\n",
    "#### Asymptotic Distribution\n",
    "\n",
    "Under the null hypothesis and standard regularity conditions, the test statistic $J_{st}$ converges in distribution to the standard normal distribution:\n",
    "\n",
    "$$\n",
    "J_{st} \\xrightarrow{d} \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "Source:\n",
    "[Allan Timmermann & Yinchu Zhu (2019). Comparing Forecasting Performance with Panel Data. SSRN Working Paper](https://rady.ucsd.edu/_files/faculty-research/timmermann/Panel%20Forecast%20Comparison%20Tests%2004_30_2019.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d119b9b",
   "metadata": {},
   "source": [
    "#### Newey–West Estimator for $\\hat{\\sigma}_{\\hat{L}}$\n",
    "\n",
    "To account for potential autocorrelation in the series of scaled average loss differentials $\\hat{L}_t$, we employ the Newey–West estimator to obtain a consistent estimate of the standard deviation $\\hat{\\sigma}_{\\hat{L}}$.\n",
    "\n",
    "Let $\\tilde{L}_t = \\hat{L}_t - \\bar{\\hat{L}}$ denote the demeaned series, where $\\bar{\\hat{L}} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{L}_t$ is the sample mean of $\\hat{L}_t$.\n",
    "\n",
    "The Newey–West estimator of the variance $\\hat{\\sigma}_{\\hat{L}}^2$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{\\hat{L}}^2 = \\sum_{j = -J}^{J} \\left(1 - \\frac{|j|}{J + 1}\\right) \\hat{\\gamma}(j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $J$ is the maximum lag length (also known as the bandwidth),\n",
    "- $\\hat{\\gamma}(j)$ is the sample autocovariance at lag $j$, defined as:\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}(j) = \\frac{1}{T} \\sum_{t = |j| + 1}^{T} \\tilde{L}_t \\tilde{L}_{t - |j|}\n",
    "$$\n",
    "\n",
    "For negative lags ($j < 0$), we set $\\hat{\\gamma}(j) = \\hat{\\gamma}(-j)$ to ensure symmetry.\n",
    "\n",
    "The weights $\\left(1 - \\frac{|j|}{J + 1}\\right)$ correspond to the Bartlett kernel, which assigns decreasing weights to autocovariances at higher lags.\n",
    "\n",
    "Finally, the standard deviation estimate is obtained by taking the square root of the variance estimate:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{\\hat{L}} = \\sqrt{\\hat{\\sigma}_{\\hat{L}}^2}\n",
    "$$\n",
    "\n",
    "This estimator provides a heteroskedasticity and autocorrelation consistent (HAC) estimate of the standard deviation, which is crucial for valid inference in the presence of autocorrelated loss differentials.\n",
    "\n",
    "Source:\n",
    "[Newey, W. K., & West, K. D. (1987). A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3), 703–708](https://users.ssc.wisc.edu/~bhansen/718/NeweyWest1987.pdf?utm_source=chatgpt.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a22e0b",
   "metadata": {},
   "source": [
    "#### This test will be used both for Mean and Variance predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fef53304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_DM(L, n):\n",
    "\n",
    "    T = len(L)\n",
    "\n",
    "    # Scale loss differentials\n",
    "    L_hat = np.sqrt(n) * L.values\n",
    "\n",
    "    # OLS with intercept only to prepare for HAC variance estimation\n",
    "    X = np.ones((T, 1))\n",
    "    ols = sm.OLS(L_hat, X).fit()\n",
    "\n",
    "    # Newey–West standard error\n",
    "    nlags = int(np.floor(4 * (T / 100) ** (2 / 9)))\n",
    "    hac_cov = cov_hac(ols, nlags=nlags)\n",
    "    se_hac = np.sqrt(hac_cov[0, 0])\n",
    "\n",
    "    # Compute test statistic and p-value\n",
    "    J_st = ols.params[0] / se_hac\n",
    "    p_value = 1 - stats.norm.cdf(J_st)\n",
    "\n",
    "    return p_value, J_st\n",
    "\n",
    "def compare_mean(train: pd.DataFrame, test: pd.DataFrame):\n",
    "\n",
    "    model = PanelOLS(train['ret'], train[['const']], entity_effects=True, time_effects=True).fit(cov_type=\"robust\")\n",
    "    \n",
    "    # Extract average time effects to use in test set predictions\n",
    "    gamma = model.estimated_effects.groupby('t').mean()['estimated_effects']\n",
    "    test['gamma'] = test.index.get_level_values('t').map(gamma)\n",
    "    test['pred_model'] = test['gamma'] + model.params['const']\n",
    "    test['e2_model'] = (test['ret'] - test['pred_model']) ** 2\n",
    "\n",
    "    # Fit restricted model with only intercept\n",
    "    model_const = PanelOLS(train['ret'], train[['const']], entity_effects=True, time_effects=False).fit(cov_type=\"robust\")\n",
    "    test['pred_const'] = model_const.params['const']\n",
    "    test['e2_const'] = (test['ret'] - test['pred_const']) ** 2\n",
    "\n",
    "    # Compute average squared error loss differential per time period\n",
    "    L = (test['e2_const'] - test['e2_model']).groupby('t').mean()\n",
    "    n = test.index.get_level_values('i').nunique()\n",
    "    \n",
    "    p_value, J_st = pooled_DM(L, n)\n",
    "\n",
    "    return p_value, J_st, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_mean(data_wide: pd.DataFrame, n: int = 5, fix_random: bool = True, train_size: float = 0.9):\n",
    "    \"\"\"\n",
    "    Perform cross-validation of the means.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    fitted_data = []\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        rand_state = i if fix_random else None\n",
    "        train_data_wide, test_data_wide = get_subsamples(data_wide, train_size=train_size, shuffle=True, random_state=rand_state)\n",
    "\n",
    "        train = to_long_format(train_data_wide)\n",
    "        test = to_long_format(test_data_wide)\n",
    "\n",
    "        p_value, J_st, model = compare_mean(train, test)\n",
    "\n",
    "        results.append({'p_value': p_value,'J_st': J_st})\n",
    "        fitted_data.append((model, train, test))\n",
    "\n",
    "    return pd.DataFrame(results), fitted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455bfe0",
   "metadata": {},
   "source": [
    "### Predictions of Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1177dca",
   "metadata": {},
   "source": [
    "Perform n splits and average $J_{st}$ to calculate pooled (bootstrap) p-value\\\n",
    "However, as $J_i$ are correlated as drawn from one sample, CLT does not apply. For this reason we will use Nadeau & Bengio t-test:\n",
    "\n",
    "$\\rho \\approx \\text{test size} $\n",
    "\n",
    "$\n",
    "\\widehat{\\mathrm{Var}}(\\bar J) = (\\frac{1}{n} + \\frac{\\rho}{1 - \\rho}) \\times \\widehat{\\mathrm{Var}}(J_i) \\quad\\text{so that}\\quad t = \\frac{\\bar{J}}{\\sqrt{\\widehat{\\mathrm{Var}}( \\bar{J} )}}\\;\\overset{d}{\\to}\\; t_{n-1}.\n",
    "$\n",
    "\n",
    "Source: [Nadeau, C., & Bengio, Y. (2003). Inference for the Generalization Error. Advances in Neural Information Processing Systems, 16, 307–313](https://proceedings.neurips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169041c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [13:41<00:00,  8.21s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_value</th>\n",
       "      <th>J_st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250506</td>\n",
       "      <td>0.672898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.477570</td>\n",
       "      <td>0.056253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.256952</td>\n",
       "      <td>0.652770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.892014</td>\n",
       "      <td>-1.237309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100437</td>\n",
       "      <td>1.279064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p_value      J_st\n",
       "0  0.250506  0.672898\n",
       "1  0.477570  0.056253\n",
       "2  0.256952  0.652770\n",
       "3  0.892014 -1.237309\n",
       "4  0.100437  1.279064"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Takes a while to run\n",
    "n = 100\n",
    "train_size=0.9\n",
    "results_mean, fitted_data_mean = cross_val_mean(R, n=n, fix_random=True, train_size=train_size)\n",
    "results_mean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21dcd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_test(J, train_size, n):\n",
    "\n",
    "    J_bar = np.mean(J)\n",
    "    rho = 1 - train_size\n",
    "\n",
    "    # Corrected variance: Var(J_bar) = (1/n + rho / (1 - rho)) * Var(J_i)\n",
    "    s2 = np.var(J, ddof=1)\n",
    "    var_corrected = (1/n + rho / (1 - rho)) * s2\n",
    "    se_corrected = np.sqrt(var_corrected)\n",
    "\n",
    "    t_stat = J_bar / se_corrected\n",
    "    p_value = 1 - stats.t.cdf(t_stat, df=n-1)  # one-sided test\n",
    "\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled p-value: 0.049791430969976735\n",
      "share of positive J_st (unrestricted model performed better): 0.73\n"
     ]
    }
   ],
   "source": [
    "J_mean = results_mean['J_st'].values\n",
    "\n",
    "p_value_mean = NB_test(J_mean, train_size, n)\n",
    "\n",
    "print('pooled p-value:', p_value_mean)\n",
    "print('share of positive J_st (unrestricted model performed better):', np.mean(J_mean > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be5147",
   "metadata": {},
   "source": [
    "### Predictions of Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f717d63",
   "metadata": {},
   "source": [
    "Perform the same test on squared residuals, for each split:\n",
    "1. Estimate $\\hat u_{it} = R_{it} - \\hat\\beta - \\hat\\lambda_i - \\hat\\gamma_t $ for train subsample\n",
    "2. Estimate $\\hat v_{jt} = R_{jt} - \\hat\\beta - \\hat\\gamma_t $ for test subsample\n",
    "3. Based on $\\hat u^2_{it}$ estimate $\\hat k_i$ and $\\hat\\sigma_t^2$\n",
    "4. Assess the predictive power of the unrestricted model: $Var(u_{it}) = k_i \\times \\sigma_t^2$ compared to the restricted model: $Var(u_{it}) = k_i$, based on the test subsample (where $k_i$ is unknown) and thus $\\widehat{Var}(v_{jt}) = \\hat k \\times \\hat \\sigma_t^2$ vs $\\widehat{Var}(v_{jt}) = \\hat k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a24d40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_variance(data):\n",
    "    \"\"\"\n",
    "    Perform cross-validation of the means.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_resid(data_, model_mean):\n",
    "        \n",
    "        data = data_.copy()\n",
    "        gamma = model_mean.estimated_effects.groupby('t').mean()['estimated_effects']\n",
    "        data['gamma'] = data.index.get_level_values('t').map(gamma)\n",
    "        data['pred'] = data['gamma'] + model_mean.params['const']\n",
    "        data['resid'] = data['ret'] - data['pred']\n",
    "\n",
    "        return data['resid']\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_mean, train_mean, test_mean in tqdm(data):\n",
    "\n",
    "        resid_train = get_resid(train_mean, model_mean)\n",
    "\n",
    "        resid_test = get_resid(test_mean, model_mean)\n",
    "        test_var = resid_test ** 2\n",
    "\n",
    "        _, sigma_train = estimate_k_sigma(resid_train.reset_index(), silent = True)\n",
    "        \n",
    "        k_restricted = test_var.mean()\n",
    "        k_unrestricted = (test_var / sigma_train).mean()\n",
    "\n",
    "        e2_restricted = (test_var - k_restricted) ** 2\n",
    "        e2_unrestricted = (test_var - k_unrestricted * sigma_train) ** 2\n",
    "\n",
    "        L = (e2_restricted - e2_unrestricted).groupby('t').mean()\n",
    "        n = test_var.index.get_level_values('i').nunique()\n",
    "        \n",
    "        p_value, J_st = pooled_DM(L, n)\n",
    "        \n",
    "        results.append({'p_value': p_value,'J_st': J_st})\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bbf077e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:05<00:00,  1.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_value</th>\n",
       "      <th>J_st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.118090</td>\n",
       "      <td>1.184588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165494</td>\n",
       "      <td>0.972127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106280</td>\n",
       "      <td>1.246559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.153295</td>\n",
       "      <td>1.022404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125364</td>\n",
       "      <td>1.148585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p_value      J_st\n",
       "0  0.118090  1.184588\n",
       "1  0.165494  0.972127\n",
       "2  0.106280  1.246559\n",
       "3  0.153295  1.022404\n",
       "4  0.125364  1.148585"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_var = cross_val_variance(fitted_data_mean).head(5)\n",
    "results_var.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44955c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled p-value: 0.0\n",
      "share of positive J_st (unrestricted model performed better): 1.0\n"
     ]
    }
   ],
   "source": [
    "J_var = results_var['J_st'].values\n",
    "\n",
    "p_value_var = NB_test(J_var, train_size, n)\n",
    "\n",
    "print('pooled p-value:', p_value_var)\n",
    "print('share of positive J_st (unrestricted model performed better):', np.mean(J_var > 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
